{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyNn13DdcwyHGSQ2spIc4Kk3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khanabdullah9/Reinforcement-Learning/blob/master/SARSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aJiuJrmhUfv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Simple Grid Environment\n",
        "# -----------------------------\n",
        "class GridWorld:\n",
        "    def __init__(self, rows=5, cols=5, goal=(4, 4), walls=[]):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.goal = goal\n",
        "        self.walls = set(walls)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = (0, 0)\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        r, c = self.state\n",
        "        if action == 0:    # up\n",
        "            r = max(r - 1, 0)\n",
        "        elif action == 1:  # down\n",
        "            r = min(r + 1, self.rows - 1)\n",
        "        elif action == 2:  # left\n",
        "            c = max(c - 1, 0)\n",
        "        elif action == 3:  # right\n",
        "            c = min(c + 1, self.cols - 1)\n",
        "\n",
        "        next_state = (r, c)\n",
        "        reward = -1\n",
        "        done = False\n",
        "\n",
        "        if next_state in self.walls:\n",
        "            reward = -5\n",
        "            next_state = self.state\n",
        "        elif next_state == self.goal:\n",
        "            reward = 10\n",
        "            done = True\n",
        "\n",
        "        self.state = next_state\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def get_state_space(self):\n",
        "        return [(r, c) for r in range(self.rows) for c in range(self.cols) if (r, c) not in self.walls]\n",
        "\n",
        "    def render(self):\n",
        "        grid = np.full((self.rows, self.cols), \"_\", dtype=str)\n",
        "        for (r, c) in self.walls:\n",
        "            grid[r, c] = \"#\"\n",
        "        r, c = self.goal\n",
        "        grid[r, c] = \"G\"\n",
        "        r, c = self.state\n",
        "        grid[r, c] = \"A\"\n",
        "        print(\"\\n\".join(\" \".join(row) for row in grid))\n",
        "        print()\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# SARSA Agent\n",
        "# -----------------------------\n",
        "class SARSAAgent:\n",
        "    def __init__(self, rows, cols, actions=[0,1,2,3], alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_min=0.05, epsilon_decay=0.995):\n",
        "        self.actions = actions\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.Q = np.zeros((rows, cols, len(actions)))\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice(self.actions)\n",
        "        r, c = state\n",
        "        return np.argmax(self.Q[r, c])\n",
        "\n",
        "    def update(self, state, action, reward, next_state, next_action):\n",
        "        r, c = state\n",
        "        nr, nc = next_state\n",
        "        predict = self.Q[r, c, action]\n",
        "        target = reward + self.gamma * self.Q[nr, nc, next_action]\n",
        "        self.Q[r, c, action] += self.alpha * (target - predict)\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Training\n",
        "# -----------------------------\n",
        "def train(env, agent, episodes=1000, max_steps=100):\n",
        "    rewards = []\n",
        "    for ep in range(episodes):\n",
        "        state = env.reset()\n",
        "        action = agent.choose_action(state)\n",
        "        total_reward = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            next_state, reward, done = env.step(action)\n",
        "            next_action = agent.choose_action(next_state)\n",
        "            agent.update(state, action, reward, next_state, next_action)\n",
        "\n",
        "            state, action = next_state, next_action\n",
        "            total_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        agent.decay_epsilon()\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        if (ep + 1) % 100 == 0:\n",
        "            avg_r = np.mean(rewards[-100:])\n",
        "            print(f\"Episode {ep+1}/{episodes} | Avg Reward (last 100): {avg_r:.2f} | Epsilon: {agent.epsilon:.3f}\")\n",
        "\n",
        "    return rewards\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Testing\n",
        "# -----------------------------\n",
        "def test(env, agent, max_steps=50):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    for step in range(max_steps):\n",
        "        env.render()\n",
        "        action = np.argmax(agent.Q[state[0], state[1]])\n",
        "        print(f\"Selected action: {action}\")\n",
        "        next_state, reward, done = env.step(action)\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "        if done:\n",
        "            print(\"Goal reached!\")\n",
        "            break\n",
        "    print(f\"Total Reward: {total_reward}\")\n"
      ],
      "metadata": {
        "id": "HZ4lyOf3hrum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = GridWorld(rows=5, cols=10, goal=(4,9))\n",
        "agent = SARSAAgent(rows=env.rows, cols=env.cols)\n",
        "train(env, agent, episodes=10_000)\n",
        "test(env, agent)"
      ],
      "metadata": {
        "id": "73gqn34zh3qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(agent.Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtlMMWQUlOqZ",
        "outputId": "8d895c70-c75b-45fb-abf0-7c705b2011c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"sarsa_q.npy\", agent.Q)"
      ],
      "metadata": {
        "id": "u6w-91dcnAsi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}